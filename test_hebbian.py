#!/usr/bin/env python3
"""
Hebbian Learning Prototype
==========================
Test: memories that are recalled together should automatically form connections.
"Neurons that fire together, wire together"
"""

from engram import Memory
from collections import defaultdict
from itertools import combinations

# æ‰©å±• Memory ç±»ï¼ŒåŠ å…¥ Hebbian learning
class HebbianMemory(Memory):
    def __init__(self, path, **kwargs):
        super().__init__(path, **kwargs)
        # è¿½è¸ª co-activation æ¬¡æ•°
        self._coactivation = defaultdict(int)  # (id1, id2) -> count
        self._hebbian_threshold = 3  # co-activate 3æ¬¡åè‡ªåŠ¨å»ºç«‹è¿æ¥
        
        # åˆ›å»º memory-to-memory è¿æ¥è¡¨
        self._store._conn.execute("""
            CREATE TABLE IF NOT EXISTS hebbian_links (
                source_id TEXT REFERENCES memories(id) ON DELETE CASCADE,
                target_id TEXT REFERENCES memories(id) ON DELETE CASCADE,
                strength REAL DEFAULT 1.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (source_id, target_id)
            )
        """)
        self._store._conn.commit()
    
    def recall(self, query, limit=5, **kwargs):
        results = super().recall(query, limit=limit, **kwargs)
        
        # è®°å½• co-activation
        ids = [r['id'] for r in results]
        for id1, id2 in combinations(ids, 2):
            # ä¿è¯é¡ºåºä¸€è‡´ (smaller_id, larger_id)
            pair = tuple(sorted([id1, id2]))
            self._coactivation[pair] += 1
            
            # è¶…è¿‡é˜ˆå€¼ â†’ åˆ›å»ºè¿æ¥
            if self._coactivation[pair] == self._hebbian_threshold:
                self._create_hebbian_link(id1, id2)
        
        return results
    
    def _create_hebbian_link(self, id1, id2):
        """åœ¨ä¸¤ä¸ªè®°å¿†ä¹‹é—´åˆ›å»º Hebbian è¿æ¥"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¿æ¥
        existing = self._store._conn.execute(
            "SELECT 1 FROM hebbian_links WHERE source_id=? AND target_id=?",
            (id1, id2)
        ).fetchone()
        
        if not existing:
            # åˆ›å»ºåŒå‘è¿æ¥
            self._store._conn.execute(
                "INSERT OR IGNORE INTO hebbian_links (source_id, target_id, strength) VALUES (?, ?, ?)",
                (id1, id2, 1.0)
            )
            self._store._conn.execute(
                "INSERT OR IGNORE INTO hebbian_links (source_id, target_id, strength) VALUES (?, ?, ?)",
                (id2, id1, 1.0)
            )
            self._store._conn.commit()
            
            # è·å–è®°å¿†å†…å®¹ç”¨äºæ‰“å°
            m1 = self._store.get(id1)
            m2 = self._store.get(id2)
            print(f"  ğŸ”— Hebbian link formed: '{m1.content[:30]}...' â†” '{m2.content[:30]}...'")
    
    def get_hebbian_links(self):
        """è·å–æ‰€æœ‰ Hebbian è¿æ¥"""
        rows = self._store._conn.execute(
            "SELECT DISTINCT source_id, target_id FROM hebbian_links"
        ).fetchall()
        return [(r[0], r[1]) for r in rows]
    
    def coactivation_stats(self):
        """è¿”å› co-activation ç»Ÿè®¡"""
        return dict(self._coactivation)


# ========== æµ‹è¯• ==========
print("=" * 60)
print("  Hebbian Learning Test")
print("=" * 60)

mem = HebbianMemory(":memory:")

# æ·»åŠ ä¸€äº›è®°å¿†ï¼ˆä¸æ‰‹åŠ¨æŒ‡å®šè¿æ¥ï¼‰
print("\nğŸ“ Adding memories (no manual links)...\n")
mem.add("Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€", type="factual", importance=0.5)
mem.add("æœºå™¨å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®", type="factual", importance=0.6)
mem.add("ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€", type="factual", importance=0.7)
mem.add("TensorFlowæ˜¯Googleçš„MLæ¡†æ¶", type="factual", importance=0.5)
mem.add("PyTorchæ›´pythonicï¼Œé€‚åˆç ”ç©¶", type="opinion", importance=0.6)
mem.add("ä»Šå¤©å¤©æ°”å¾ˆå¥½", type="episodic", importance=0.1)
mem.add("å’–å•¡å¸®åŠ©æˆ‘é›†ä¸­æ³¨æ„åŠ›", type="relational", importance=0.3)

print(f"Total memories: {len(mem)}")
print(f"Initial Hebbian links: {len(mem.get_hebbian_links())}")

# æ¨¡æ‹Ÿå¤šæ¬¡æŸ¥è¯¢ï¼Œè§‚å¯Ÿè¿æ¥å½¢æˆ
print("\nğŸ” Simulating queries...\n")

queries = [
    "æœºå™¨å­¦ä¹ æ¡†æ¶",      # Should recall TensorFlow, PyTorch, ç¥ç»ç½‘ç»œ
    "æ·±åº¦å­¦ä¹ å·¥å…·",      # Similar - should reinforce same connections
    "Python ML",        # Should recall Python, ML related
    "ç¥ç»ç½‘ç»œ PyTorch",  # Reinforce connections
    "æœºå™¨å­¦ä¹ ",         # More reinforcement
]

for i, q in enumerate(queries, 1):
    print(f"\n--- Query {i}: '{q}' ---")
    results = mem.recall(q, limit=4)
    for r in results:
        print(f"  [{r['type'][:4]}] {r['content'][:50]}")

# æ˜¾ç¤ºæœ€ç»ˆç»“æœ
print("\n" + "=" * 60)
print("  Results")
print("=" * 60)

print(f"\nğŸ“Š Co-activation stats:")
stats = mem.coactivation_stats()
sorted_stats = sorted(stats.items(), key=lambda x: x[1], reverse=True)
for pair, count in sorted_stats[:10]:
    m1 = mem._store.get(pair[0])
    m2 = mem._store.get(pair[1])
    print(f"  {count}x: '{m1.content[:25]}...' â†” '{m2.content[:25]}...'")

print(f"\nğŸ”— Hebbian links formed: {len(mem.get_hebbian_links()) // 2}")  # é™¤2å› ä¸ºæ˜¯åŒå‘

# æµ‹è¯•ï¼šgraph_expand ç°åœ¨åº”è¯¥èƒ½åˆ©ç”¨ Hebbian links
print("\nğŸ§  Testing graph expansion with Hebbian links...")
print("\n--- Query: 'TensorFlow' (with graph_expand=True) ---")
results = mem.recall("TensorFlow", limit=5, graph_expand=True)
for r in results:
    print(f"  [{r['type'][:4]}] {r['content'][:50]}")

print("\nâœ… Hebbian learning works!")
